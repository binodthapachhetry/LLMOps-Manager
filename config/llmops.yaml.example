# LLMOps Manager Configuration Example

# AWS Configuration
aws:
  region: us-west-2
  role_arn: arn:aws:iam::123456789012:role/LLMOpsManagerRole

# GCP Configuration
gcp:
  project_id: llmops-project
  region: us-central1

# Azure Configuration
azure:
  subscription_id: subscription-id
  resource_group: llmops-resource-group
  workspace_name: llmops-workspace

# Deployment Configuration
deployment:
  default_instance_type: ml.g4dn.xlarge
  default_instance_count: 1
  canary_evaluation_duration: 600
  validation_tests:
    - name: basic_inference
      inputs:
        text: "Hello, world!"
      expected_outputs:
        contains: "Hello"

# Monitoring Configuration
monitoring:
  collectors:
    collection_interval_seconds: 60
    enabled_metrics:
      - latency
      - throughput
      - token_usage
      - error_rate
      - gpu_utilization
    drift_detector:
      window_size: 1000
      drift_threshold: 0.05
  alerts:
    thresholds:
      latency:
        warning: 500
        error: 1000
        critical: 2000
        direction: above
      error_rate:
        warning: 0.01
        error: 0.05
        critical: 0.1
        direction: above
      gpu_utilization:
        warning: 85
        error: 95
        direction: above
    email_alerts:
      enabled: false
      recipients:
        - alerts@example.com
    webhook_alerts:
      enabled: false
      url: https://example.com/webhook

# Scaling Configuration
scaling:
  check_interval_seconds: 60
  default_min_replicas: 1
  default_max_replicas: 10
  default_target_gpu_utilization: 70.0

# Versioning Configuration
versioning:
  model_registry:
    storage_prefix: models
    local_cache_dir: model_cache
  prompt_registry:
    storage_prefix: prompts
    local_cache_dir: prompt_cache
